{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single step test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "import pyqg_explorer.dataset.forcing_dataset as forcing_dataset\n",
    "import pyqg_explorer.models.base_model as base_model\n",
    "import pyqg_explorer.util.pbar as pbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPUs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev=0\n",
    "data_full=xr.open_zarr(fsspec.get_mapper(f'/scratch/zanna/data/pyqg/publication/eddy/forcing1.zarr'), consolidated=True)\n",
    "data_dqbar=data_full.dqbar_dt.isel(lev=0)\n",
    "data_dqbar=data_dqbar.stack(snapshot=(\"run\",\"time\"))\n",
    "data_dqbar=data_dqbar.transpose(\"snapshot\",\"y\",\"x\")\n",
    "data_dqbar.isel(snapshot=200).plot()\n",
    "data_forcing=data_full.q_subgrid_forcing.isel(lev=lev)\n",
    "data_forcing=data_forcing.stack(snapshot=(\"run\",\"time\"))\n",
    "data_forcing=data_forcing.transpose(\"snapshot\",\"y\",\"x\")\n",
    "data_forcing.isel(snapshot=20).plot()\n",
    "data_q=data_full.q.isel(lev=0)\n",
    "data_q=data_q.stack(snapshot=(\"run\",\"time\"))\n",
    "data_q=data_q.transpose(\"snapshot\",\"y\",\"x\")\n",
    "data_q.isel(snapshot=20).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dqbar=data_full.dqbar_dt.isel(lev=0)\n",
    "data_dqbar=data_dqbar.stack(snapshot=(\"run\",\"time\"))\n",
    "data_dqbar=data_dqbar.transpose(\"snapshot\",\"y\",\"x\")\n",
    "data_dqbar.isel(snapshot=200).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_forcing=data_full.q_subgrid_forcing.isel(lev=lev)\n",
    "data_forcing=data_forcing.stack(snapshot=(\"run\",\"time\"))\n",
    "data_forcing=data_forcing.transpose(\"snapshot\",\"y\",\"x\")\n",
    "data_forcing.isel(snapshot=20).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_q=data_full.q.isel(lev=0)\n",
    "data_q=data_q.stack(snapshot=(\"run\",\"time\"))\n",
    "data_q=data_q.transpose(\"snapshot\",\"y\",\"x\")\n",
    "data_q.isel(snapshot=20).plot()\n",
    "\n",
    "del data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a single-step dataset\n",
    "class SingleStepDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Subgrid forcing maps dataset\n",
    "    \"\"\"\n",
    "    def __init__(self,pv,dqbar_dt,s,seed=42,train_ratio=0.75,valid_ratio=0.25,test_ratio=0.0):\n",
    "        \"\"\"\n",
    "        pv:          xarray of the PV field\n",
    "        dqbar_dt:    xarray of PV tendency\n",
    "        s:           xarray of the subgrid forcing field\n",
    "        seed:        random seed used to create train/valid/test splits\n",
    "        train_ratio: proportion of dataset to use as training data\n",
    "        valid_ratio: proportion of dataset to use as validation data\n",
    "        test_ratio:  proportion of dataset to use as test data\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pv=torch.unsqueeze(torch.tensor(pv.to_numpy()),dim=1)\n",
    "        self.dqbar_dt=torch.unsqueeze(torch.tensor(dqbar_dt.to_numpy()),dim=1)\n",
    "        self.s=torch.unsqueeze(torch.tensor(s.to_numpy()),dim=1)\n",
    "        ## Generate array for Q_i+1\n",
    "        self.pv_plusone=torch.roll(self.pv,1,dims=0)\n",
    "        \n",
    "        ## Drop last index, where we have no i+1\n",
    "        self.pv=self.pv[:-1, :, :, :]\n",
    "        self.dqbar_dt=self.dqbar_dt[:-1, :, :, :]\n",
    "        self.s=self.s[:-1, :, :, :]\n",
    "        self.pv_plusone=self.pv_plusone[1:, :, :, :]\n",
    "        \n",
    "        ## Cat into x_data\n",
    "        self.x_data=torch.cat((self.pv,self.dqbar_dt,self.s),1)\n",
    "        self.y_data=self.pv_plusone\n",
    "        \n",
    "        self.train_ratio=train_ratio\n",
    "        self.valid_ratio=valid_ratio\n",
    "        self.test_ratio=test_ratio\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.x_renorm=torch.std(self.x_data)\n",
    "        self.y_renorm=torch.std(self.y_data)\n",
    "        self.x_data=self.x_data/self.x_renorm\n",
    "        self.y_data=self.y_data/self.y_renorm\n",
    "        self.len=len(self.x_data)\n",
    "        \n",
    "        assert len(self.x_data)==len(self.y_data), \"Number of x and y samples should be the same\"\n",
    "        \n",
    "        self._get_split_indices()\n",
    "        \n",
    "    def _get_split_indices(self):\n",
    "        \"\"\" Set indices for train, valid and test splits \"\"\"\n",
    "\n",
    "        ## Randomly shuffle indices of entire dataset\n",
    "        rand_indices=self.rng.permutation(np.arange(self.len))\n",
    "\n",
    "        ## Set number of train, valid and test points\n",
    "        num_train=math.floor(self.len*self.train_ratio)\n",
    "        num_valid=math.floor(self.len*self.valid_ratio)\n",
    "        num_test=math.floor(self.len*self.test_ratio)\n",
    "        \n",
    "        ## Make sure we aren't overcounting\n",
    "        assert (num_train+num_valid+num_test) <= self.len\n",
    "        \n",
    "        ## Pick train, test and valid indices from shuffled list\n",
    "        self.train_idx=rand_indices[0:num_train]\n",
    "        self.valid_idx=rand_indices[num_train+1:num_train+num_valid]\n",
    "        self.test_idx=rand_indices[len(self.valid_idx)+1:]\n",
    "        \n",
    "        ## Make sure there's no overlap between train, valid and test data\n",
    "        assert len(set(self.train_idx) & set(self.valid_idx) & set(self.test_idx))==0, (\n",
    "                \"Common elements in train, valid or test set\")\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return (self.x_data[idx],self.y_data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Andrew/Pavel's code, function to create a CNN block\n",
    "def make_block(in_channels: int, out_channels: int, kernel_size: int, \n",
    "        ReLU = 'ReLU', batch_norm = True) -> list:\n",
    "    '''\n",
    "    Packs convolutional layer and optionally ReLU/BatchNorm2d\n",
    "    layers in a list\n",
    "    '''\n",
    "    conv = nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "        padding='same', padding_mode='circular')\n",
    "    block = [conv]\n",
    "    if ReLU == 'ReLU':\n",
    "        block.append(nn.ReLU())\n",
    "    elif ReLU == 'LeakyReLU':\n",
    "        block.append(nn.LeakyReLU(0.2))\n",
    "    elif ReLU == 'False':\n",
    "        pass\n",
    "    else:\n",
    "        print('Error: wrong ReLU parameter')\n",
    "    if batch_norm:\n",
    "        block.append(nn.BatchNorm2d(out_channels))\n",
    "    return block\n",
    "\n",
    "\n",
    "class AndrewCNN(nn.Module):\n",
    "    def __init__(self, n_in: int, n_out: int, x_renorm=torch.tensor(1.), y_renorm=torch.tensor(1.), ReLU = 'ReLU', lr=0.001) -> list:\n",
    "        '''\n",
    "        Packs sequence of 8 convolutional layers in a list.\n",
    "        First layer has n_in input channels, and Last layer has n_out\n",
    "        output channels\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.lr=lr\n",
    "        ## Register normalisation factors as buffers\n",
    "        self.register_buffer('x_renorm', x_renorm)\n",
    "        self.register_buffer('y_renorm', y_renorm)\n",
    "        blocks = []\n",
    "        blocks.extend(make_block(n_in,128,5,ReLU))                #1\n",
    "        blocks.extend(make_block(128,64,5,ReLU))                  #2\n",
    "        blocks.extend(make_block(64,32,3,ReLU))                   #3\n",
    "        blocks.extend(make_block(32,32,3,ReLU))                   #4\n",
    "        blocks.extend(make_block(32,32,3,ReLU))                   #5\n",
    "        blocks.extend(make_block(32,32,3,ReLU))                   #6\n",
    "        blocks.extend(make_block(32,32,3,ReLU))                   #7\n",
    "        blocks.extend(make_block(32,n_out,3,'False',False))       #8\n",
    "        self.conv = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dataset=SingleStepDataset(data_q,data_dqbar,data_forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wandb config file\n",
    "config={\"lev\":lev,\n",
    "        \"forcing\":1,\n",
    "        \"framework\":\"Single-step loss\"}\n",
    "\n",
    "wandb.init(project=\"pyqg_single_step\", entity=\"chris-pedersen\",config=config)\n",
    "train_loader = DataLoader(\n",
    "    single_dataset,\n",
    "    batch_size=64,\n",
    "    sampler=SubsetRandomSampler(single_dataset.train_idx),\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    single_dataset,\n",
    "    batch_size=64,\n",
    "    sampler=SubsetRandomSampler(single_dataset.valid_idx),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_theta=base_model.AndrewCNN(1,1,single_dataset.x_renorm,single_dataset.y_renorm)\n",
    "model_beta=base_model.AndrewCNN(2,1,single_dataset.x_renorm,single_dataset.y_renorm)\n",
    "\n",
    "model_theta.to(device)\n",
    "model_beta.to(device)\n",
    "\n",
    "wandb.watch([model_theta,model_beta], log_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "lr = 0.01\n",
    "wd = 0.05\n",
    "\n",
    "optimizer = torch.optim.AdamW(list(model_theta.parameters()) + list(model_beta.parameters()), lr=lr, weight_decay=wd, betas=(beta1, beta2))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)\n",
    "\n",
    "criterion=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(150):  # loop over the dataset multiple times\n",
    "\n",
    "    train_running_loss = 0.0\n",
    "    train_samples = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    valid_samples = 0.0\n",
    "    \n",
    "    model_theta.train()\n",
    "    model_beta.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        ## x_data is ordered in [pv, dqdt, s]\n",
    "        x_data, y_data = data\n",
    "        x_data=x_data.to(device)\n",
    "        y_data=y_data.to(device)\n",
    "        \n",
    "        #print(x_data)\n",
    "        #print(x_data[:,2,:,:].unsqueeze(1))\n",
    "    \n",
    "        ## zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## First network\n",
    "        output_theta = model_theta(x_data[:,2,:,:].unsqueeze(1)) ## Takes in PV, outputs S\n",
    "        ## Now evaluate F+\\hat{S}, the tendency + estimated forcing\n",
    "        F_plus_s_hat = output_theta+x_data[:,1,:,:].unsqueeze(1)\n",
    "        ## Second network\n",
    "        output_beta = model_beta(torch.cat((x_data[:,1,:,:].unsqueeze(1),F_plus_s_hat),1))\n",
    "        loss_1 = criterion(output_theta, x_data[2])\n",
    "        loss_2 = criterion(output_beta, y_data)\n",
    "        loss = loss_1+loss_2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## Track loss for wandb\n",
    "        train_running_loss=+loss\n",
    "        train_samples+=x_data.shape[0]\n",
    "    \n",
    "    model_theta.eval()\n",
    "    model_beta.eval()\n",
    "    for i, data in enumerate(valid_loader, 0):\n",
    "        ## x_data is ordered in [pv, dqdt, s]\n",
    "        x_data, y_data = data\n",
    "        x_data=x_data.to(device)\n",
    "        y_data=y_data.to(device)\n",
    "    \n",
    "        ## zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## First network\n",
    "        output_theta = model_theta(x_data[:,2,:,:].unsqueeze(1)) ## Takes in PV, outputs S\n",
    "        ## Now evaluate F+\\hat{S}, the tendency + estimated forcing\n",
    "        F_plus_s_hat = output_theta+x_data[:,1,:,:].unsqueeze(1)\n",
    "        ## Second network\n",
    "        output_beta = model_beta(torch.cat((x_data[:,1,:,:].unsqueeze(1),F_plus_s_hat),1))\n",
    "        val_loss_1 = criterion(output_theta, x_data[2])\n",
    "        val_loss_2 = criterion(output_beta, y_data)\n",
    "        val_loss = val_loss_1+val_loss_2\n",
    "        ## Track loss for wandb\n",
    "        valid_running_loss=+loss\n",
    "        valid_samples+=x_data.shape[0]\n",
    "    \n",
    "    log_dic={}\n",
    "    log_dic[\"training_loss\"]=train_running_loss/train_samples\n",
    "    log_dic[\"valid_loss\"]=valid_running_loss/valid_samples\n",
    "    wandb.log(log_dic)\n",
    "    \n",
    "    # verbose\n",
    "    print('%03d %.3e %.3e '%(epoch, train_running_loss/train_samples, valid_running_loss), end='')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:,1,:,:].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_theta = model_theta(x_train[:,1,:,:].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_plus_shat=output_theta+x_train[:,1,:,:].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_plus_shat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((x_train[:,1,:,:].unsqueeze(1),F_plus_shat),1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_beta = model_beta(torch.cat((x_train[:,1,:,:].unsqueeze(1),F_plus_shat),1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=(model_beta()-y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbLogger()\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=\"/scratch/cp3759/pyqg_data/models\",\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=150,\n",
    "    callbacks=pbar.ProgressBar(),\n",
    "    logger=WandbLogger()\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/scratch/cp3759/pyqg_data/models/cnn_1step_upper.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv=torch.unsqueeze(torch.tensor(data_q.to_numpy()),dim=1)\n",
    "dqbar_dt=torch.unsqueeze(torch.tensor(data_dqbar.to_numpy()),dim=1)\n",
    "s=torch.unsqueeze(torch.tensor(data_forcing.to_numpy()),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_plusone=torch.roll(pv,1,dims=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv=pv[:-1, :, :, :]\n",
    "pv_plusone=pv_plusone[1:, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=s[:-1, :, :, :]\n",
    "dqbar_dt=dqbar_dt[:-1, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((pv,dqbar_dt,s),1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sing",
   "language": "python",
   "name": "sing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
